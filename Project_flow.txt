1. create virtual env:
conda activate document_portal_llmops1
conda activate D:\LLMOPS\DOCUMENT_PORTAL\document_portal_llmops

2. pip list 

Package    Version
---------- -------
pip        25.1
setuptools 78.1.1
wheel      0.45.1

    add env in gitignore and initialize git

3.  git commit --> publish branch 

4.   Problem Statement (Document Portal)
	    -   Deployment of RAG based application
	    -	RAG Architecture
	    -	Development --> Deployment to AWS Bedrock

    Either a RAG architecture or agents Architecture

    Document Portal and intelligent chat:
	- Analyse any document
	- Compare 2 document
	- Talk with single document
	- Talk with multiple document

5.  Tools and framework(tech stack):

    Pre-requisite study:

    i.  GenAI-LLM fundamental
    ii.  LLM fine tuning
    iii.  Langchain --> Framework
    iv.  RAG   --> (Retrieval-Augmented Generation)

6.  Min Req:

    i. LLM model [grok(free), OpenAI, gemini, claude, olama(local setup), huggingface(free)] **
    ii. Embedding model[openAI, huggingface, gemini]
    iii. Vector Database: 
        -   In-memory vector Database Eg: ** (FAISS)
        -   On Disc vector Database   Eg: **
        -   Cloud based vector Database  Eg: ** ( AWS bedrock)
    
    Using these 3 we will create the RAG Pipeline.

7. setup.py
    find_packages() will check each and every folder, and inside which even folder it will find __init__.py, it will include that folder inside DOCUMENT_PORTAL Package. I want to install
    DOCUMENT_PORTAL as a package inside my current virtual env.
    pip install -e .

8. LangChain package
    Langgraph package
    using setup_tool we can convret a project into a package

9. pip install -r requirement.txt

10. groq_api_key and google_api_key

11. Logging Module:
    -   structlog
    -   log guru
    -   logging 

12. Setup logging and exception module

custom_logger.py (old: development format of logger)
custom_logger.py( the way logs should be generated for cloud configuration)

13. We place new PDF under data/document_analysis/ post execution file will be copied to <session_uuid> folder.
        -   if we dont do this it will create data redundancy in vector DB.
        -   Never delete the data, we should do data archive

S3:
    hot storage
    cold storage --> archival pdfs --> analysis using Athena/glue/redshift 
    warm storage
    archival data 

14. Project Flow:

# Data ingestion: PDF repo --> load the PDF --> Save files in session folder(archival) <def save_pdf()> and load document < read_pdf() > load the content of PDF.
# Context of PDF File --> Analysis using LLM capability --> <Prompt and Pydantic data validation> --> Create a chain --> Desired output

---------------------------------------------------------------------------------------


15. https://python.langchain.com/docs/introduction/ (V0.3)

12. Leader board:
https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard

16. LCEL - Lanchain Expression Language
     rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
Concept:

a.  Question at Runtime
        rag_chain.invoke("What is a data lake?")
The RunnablePassthrough() ensures that the exact question is preserved and passed down the chain.

b.  Vector DB Retrieval
        The question is sent to the retriever, which sits on top of a vector database (like FAISS, Pinecone, Chroma, etc.).

        The retriever does:
            -   Embedding the input question.
            -   Finding top-k similar document chunks based on vector similarity.

c.  format_docs:
        Likely flattens and formats the retrieved documents, e.g., joins text with separators.

d.  Prompt Template for Prompt
        Question
        context

e.   LLM Call
        The formatted prompt is sent to the model (LLM like GPT-4, Claude, etc.).

        The LLM uses:
            The retrieved context if it's relevant.
            Its pretrained knowledge if the context is missing or vague ‚Äî depending on how the prompt is phrased.

f.  Output Parsing
        StrOutputParser() takes the raw LLM response (which might be a message object) and returns a plain string.
        This is the final answer you receive.

17. Document Compare:

Compare between 2 document content

Doc 1: 4 lines (V1)
Doc 2:

Doc 1: 4 lines + 2 lines (V2)

Compare between Doc 1(V1) vs Doc 1(V2)

Doc 1: OPen AI pdf
Doc 2: Llama pdf

Comparison between Doc 1 vs Doc 2

Data_ingestion.py --> Read PDF and return combine txt.
Document_compare.py --> 

embedding not required

PDF --> text --> combine txt --> LLM --> output parser

18. 3rd August
single document chat --> RAG
multi document chat --> multiple PDFs

Evaluation
Memory
Cache 
Gaurdrails

19. pymupdf vs fitz vs pypdfreader

##To be discussed:
- pydantic object and output parser
- history or memory
- Cache
- token counter (cost analysis)
- pytest
- Evaluation
- gaurdrails

The FAISS index will be created only once.

20. Multi-document chat with RAG pipeline:

<.pdf, .txt, .docx> --> chunks(semantic chunking/recursive character splitter) --> embedding(small/medium/large dimension) --> vector DB (FAISS/pinecone/chromaDB) in the form of document object.
Document object --> page content and metadata.

Query --> retrieve relevant data from vectorDB stored as index. We have have 1 index or multiple index
index will have document object --> page content and metadata.

Vector operation(similarity search) between 2 vectors (cosine similarity, dot product, jaccard similarity, L1 and L2 distances)

Advance retriever technique:
1. MMR - Maximum Marginal relevance
2. Rag fusion
3. HYDE
4. Contextual compression retrieval

<Relevant Data, query, prompt> --> LLM --> output 

Advance:
1. from which document is the answer fetching from ?
2. 


uvicorn main:app --reload

21. Preparation for Deployment
- Dev Branch(enhancements) and Main Branch (Deploy to AWS)
- AWS configuration
- S3| ECR| Fargate| DynamoDB| AWS Secret Manager| Aws CW | CLoud Formation
- MMR, Advance Retrieving Technique, Cache Memory, test Cases

By Default the llm_rpovider is groq to change to google we need to set:
set LLM_PROVIDER = 'google'


Deployment:

ECS Launch Type:
1. Fargate --> AWS Managed
2. EC2  --> User Managed

## Workflow:

1. Push to git hub main branch
2. Trigger Github actions(aws.yaml)
3. Build Docker image
4. Push to Amazon ECR
5. Task definition JSON inject new image
6. ECS service updated
7. App running on Fargate

## Infra setup on cloud:

1. ECR Repo VPC(with 2 public subnets)
2. Internet Gateway
3. ECS Cluster
4. Task definition
5. ECS Service( no load Balancer)
6. IAM Role
7. Security Group

[CI ---> CD ---> CD]

CI --> Updating code base, running unit test case, github actions/jenkins --> CI.yaml
CD --> Docker image
        - ECR
        - Dockerhub
CD --> 
    Docker on EC2(Server)
    ECS + Fargate (Serverless)

Unit Test Cases
Integration Test Case





    



#####################################################################################################################
Crash course:

Retrieval-Augmented Generation (RAG) as a formal architecture was introduced by Facebook AI (now Meta AI) in a 2020 research paper

1. RAG (Retrieval-Augmented Generation)
        -   Not a framework, but a technique or architecture.
        -   What it is: A method that combines traditional retrieval (like searching a knowledge base) with a language model (like GPT).
        -   Goal: Help the model answer questions using external documents, so it doesn't rely only on what it was trained on.
        -   Common use: Chatbots, document Q&A, customer support, etc.

You can build RAG systems using various tools/frameworks ‚Äî including LangChain, LlamaIndex, Haystack, etc.

2.  LangChain
        -   it's a framework (Python & JavaScript).
        -   What it is: A high-level framework that helps you build applications with LLMs.
        -   Supports: Building RAG systems, agents, chains, tool use, memory, etc.

    Plug-and-play with:
        -   Vector stores (e.g., Pinecone, FAISS)
        -   Embedding models
        -   Language models (e.g., OpenAI, Anthropic, Cohere)

#####################################################################################################################

Example of a RAG application:

Suppose you have a policy document stored using embeddings.

User asks:
    -   What‚Äôs our refund policy if a customer cancels after 30 days?

Workflow:
    Embedding model:
    -   Converts question ‚Üí vector
    -   Finds chunk from document:
    -   "Customers are eligible for partial refunds if the cancellation occurs between 30 and 60 days."

    LLM:
    -   Reads the chunk.
    -   Understands the context.

Generates:
    -   ‚ÄúIf a customer cancels after 30 days but before 60, they may receive a partial refund as per company policy.‚Äù
    -   The LLM understands "after 30 days" maps to "between 30 and 60", and turns it into a helpful sentence.

########################################################################################################################

How Langchain helps in RAG application:

1. Document Loading & Preprocessing
    LangChain provides built-in tools to:
        -   Read from PDFs, websites, databases, s3, etc.
        -   Split documents into chunks (e.g., 500 tokens with overlap).
        -   Preprocess and clean text before embedding.

‚úÖ Saves you time from writing custom loaders and chunkers.

üß† 2. Embeddings + Vector Stores
    LangChain integrates with:
        -   Embedding models: OpenAI, Google, Cohere, HuggingFace, etc.
        -   Vector DBs: FAISS, Pinecone, Chroma, Weaviate, etc.


üîç 3. Retrieval Chain
    LangChain makes it easy to:
        -   Embed the user query
        -   Fetch relevant chunks from the vector DB
        -   Feed them into a prompt template for the LLM


üí¨ 4. Prompt Templates & Memory
    LangChain supports:
        -   Prompt engineering (system + user instructions)
        -   Chat history memory for conversation continuity
        -   Dynamic prompt construction based on input + retrieved docs

‚úÖ Keeps prompts structured and extensible.

‚öôÔ∏è 5. Tool & Agent Support (Advanced)
    For more advanced use cases, LangChain can:

        -   Add agents that decide what tools (search, calculator, database) to use.
        -   Integrate multiple steps like search ‚Üí read ‚Üí summarize ‚Üí reply.

üß± Summary of LangChain‚Äôs Role in RAG

üìÑ Document Ingestion	---->       Load, split, clean, store
üî° Embedding	        ---->       Plug into any embedding model
üß† Retrieval	        ---->       Query vector DBs easily
üí¨ Generation	        ---->       Format and run prompts with LLMs
üß† Agents (optional)	---->       Add reasoning or tool-using chains

LangChain helps you stitch together the RAG pipeline end-to-end, making it easy to connect [documents], [embeddings], [vector stores], and [LLMs] into a [working app] ‚Äî without having to write all the glue code yourself. Think of it as the ‚Äúmiddleware‚Äù for modern AI apps.

########################################################################################################################







Ask ChatGPT
