1. create virtual env:
conda activate document_portal_llmops1

2. pip list 

Package    Version
---------- -------
pip        25.1
setuptools 78.1.1
wheel      0.45.1

    add env in gitignore and initialize git

3.  git commit --> publish branch 

4.   Problem Statement (Document Portal)
	    -   Deployment of RAG based application
	    -	RAG Architecture
	    -	Development --> Deployment to AWS Bedrock

    Either a RAG architecture or agents Architecture

    Document Portal and intelligent chat:
	- Analyse any document
	- Compare 2 document
	- Talk with single document
	- Talk with multiple document

5.  Tools and framework(tech stack):

    Pre-requisite study:

    i.  GenAI-LLM fundamental
    ii.  LLM fine tuning
    iii.  Langchain --> Framework
    iv.  RAG   --> (Retrieval-Augmented Generation)

6.  Min Req:

    i. LLM model [grok(free), OpenAI, gemini, claude, olama(local setup), huggingface(free)] **
    ii. Embedding model[openAI, huggingface, gemini]
    iii. Vector Database: 
        -   In-memory vector Database Eg: ** (FAISS)
        -   On Disc vector Database   Eg: **
        -   Cloud based vector Database  Eg: ** ( AWS bedrock)
    
    Using these 3 we will create the RAG Pipeline.

7. setup.py
    find_packages() will check each and every folder, and inside which even folder it will find __init__.py, it will include that folder inside DOCUMENT_PORTAL Package. I want to install
    DOCUMENT_PORTAL as a package inside my current virtual env.
    pip install -e .

8. LangChain package
    Langgraph package
    using setup_tool we can convret a project into a package

9. pip install -r requirement.txt

10. groq_api_key and google_api_key





20th July agenda:

1. Dive into the RAG architecture and key components
2. Recap the project use case to ensure clarity
3. hand on implementation of the RAG workflow
4. configure logging and exception handling module
5. config and utils



    



#############################################
Crash course:

1. RAG (Retrieval-Augmented Generation)
        -   Not a framework, but a technique or architecture.
        -   What it is: A method that combines traditional retrieval (like searching a knowledge base) with a language model (like GPT).
        -   Goal: Help the model answer questions using external documents, so it doesn't rely only on what it was trained on.
        -   Common use: Chatbots, document Q&A, customer support, etc.

You can build RAG systems using various tools/frameworks â€” including LangChain, LlamaIndex, Haystack, etc.

2.  LangChain
        -   it's a framework (Python & JavaScript).
        -   What it is: A high-level framework that helps you build applications with LLMs.
        -   Supports: Building RAG systems, agents, chains, tool use, memory, etc.

    Plug-and-play with:
        -   Vector stores (e.g., Pinecone, FAISS)
        -   Embedding models
        -   Language models (e.g., OpenAI, Anthropic, Cohere)

#############################################