1. create virtual env:
conda activate document_portal_llmops1
conda activate D:\LLMOPS\DOCUMENT_PORTAL\document_portal_llmops

2. pip list 

Package    Version
---------- -------
pip        25.1
setuptools 78.1.1
wheel      0.45.1

    add env in gitignore and initialize git

3.  git commit --> publish branch 

4.   Problem Statement (Document Portal)
	    -   Deployment of RAG based application
	    -	RAG Architecture
	    -	Development --> Deployment to AWS Bedrock

    Either a RAG architecture or agents Architecture

    Document Portal and intelligent chat:
	- Analyse any document
	- Compare 2 document
	- Talk with single document
	- Talk with multiple document

5.  Tools and framework(tech stack):

    Pre-requisite study:

    i.  GenAI-LLM fundamental
    ii.  LLM fine tuning
    iii.  Langchain --> Framework
    iv.  RAG   --> (Retrieval-Augmented Generation)

6.  Min Req:

    i. LLM model [grok(free), OpenAI, gemini, claude, olama(local setup), huggingface(free)] **
    ii. Embedding model[openAI, huggingface, gemini]
    iii. Vector Database: 
        -   In-memory vector Database Eg: ** (FAISS)
        -   On Disc vector Database   Eg: **
        -   Cloud based vector Database  Eg: ** ( AWS bedrock)
    
    Using these 3 we will create the RAG Pipeline.

7. setup.py
    find_packages() will check each and every folder, and inside which even folder it will find __init__.py, it will include that folder inside DOCUMENT_PORTAL Package. I want to install
    DOCUMENT_PORTAL as a package inside my current virtual env.
    pip install -e .

8. LangChain package
    Langgraph package
    using setup_tool we can convret a project into a package

9. pip install -r requirement.txt

10. groq_api_key and google_api_key

11. Logging Module:
    -   structlog
    -   log guru
    -   logging 

12. Setup logging and exception module

custom_logger.py (old: development format of logger)
custom_logger.py( the way logs should be generated for cloud configuration)




---------------------------------------------------------------------------------------


11. https://python.langchain.com/docs/introduction/ (V0.3)

12. Leader board:
https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard

13. LCEL - Lanchain Expression Language
     rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
Concept:

a.  Question at Runtime
        rag_chain.invoke("What is a data lake?")
The RunnablePassthrough() ensures that the exact question is preserved and passed down the chain.

b.  Vector DB Retrieval
        The question is sent to the retriever, which sits on top of a vector database (like FAISS, Pinecone, Chroma, etc.).

        The retriever does:
            -   Embedding the input question.
            -   Finding top-k similar document chunks based on vector similarity.

c.  format_docs:
        Likely flattens and formats the retrieved documents, e.g., joins text with separators.

d.  Prompt Template for Prompt
        Question
        context

e.   LLM Call
        The formatted prompt is sent to the model (LLM like GPT-4, Claude, etc.).

        The LLM uses:
            The retrieved context if it's relevant.
            Its pretrained knowledge if the context is missing or vague ‚Äî depending on how the prompt is phrased.

f.  Output Parsing
        StrOutputParser() takes the raw LLM response (which might be a message object) and returns a plain string.
        This is the final answer you receive.



    



#####################################################################################################################
Crash course:

Retrieval-Augmented Generation (RAG) as a formal architecture was introduced by Facebook AI (now Meta AI) in a 2020 research paper

1. RAG (Retrieval-Augmented Generation)
        -   Not a framework, but a technique or architecture.
        -   What it is: A method that combines traditional retrieval (like searching a knowledge base) with a language model (like GPT).
        -   Goal: Help the model answer questions using external documents, so it doesn't rely only on what it was trained on.
        -   Common use: Chatbots, document Q&A, customer support, etc.

You can build RAG systems using various tools/frameworks ‚Äî including LangChain, LlamaIndex, Haystack, etc.

2.  LangChain
        -   it's a framework (Python & JavaScript).
        -   What it is: A high-level framework that helps you build applications with LLMs.
        -   Supports: Building RAG systems, agents, chains, tool use, memory, etc.

    Plug-and-play with:
        -   Vector stores (e.g., Pinecone, FAISS)
        -   Embedding models
        -   Language models (e.g., OpenAI, Anthropic, Cohere)

#####################################################################################################################

Example of a RAG application:

Suppose you have a policy document stored using embeddings.

User asks:
    -   What‚Äôs our refund policy if a customer cancels after 30 days?

Workflow:
    Embedding model:
    -   Converts question ‚Üí vector
    -   Finds chunk from document:
    -   "Customers are eligible for partial refunds if the cancellation occurs between 30 and 60 days."

    LLM:
    -   Reads the chunk.
    -   Understands the context.

Generates:
    -   ‚ÄúIf a customer cancels after 30 days but before 60, they may receive a partial refund as per company policy.‚Äù
    -   The LLM understands "after 30 days" maps to "between 30 and 60", and turns it into a helpful sentence.

########################################################################################################################

How Langchain helps in RAG application:

1. Document Loading & Preprocessing
    LangChain provides built-in tools to:
        -   Read from PDFs, websites, databases, s3, etc.
        -   Split documents into chunks (e.g., 500 tokens with overlap).
        -   Preprocess and clean text before embedding.

‚úÖ Saves you time from writing custom loaders and chunkers.

üß† 2. Embeddings + Vector Stores
    LangChain integrates with:
        -   Embedding models: OpenAI, Google, Cohere, HuggingFace, etc.
        -   Vector DBs: FAISS, Pinecone, Chroma, Weaviate, etc.


üîç 3. Retrieval Chain
    LangChain makes it easy to:
        -   Embed the user query
        -   Fetch relevant chunks from the vector DB
        -   Feed them into a prompt template for the LLM


üí¨ 4. Prompt Templates & Memory
    LangChain supports:
        -   Prompt engineering (system + user instructions)
        -   Chat history memory for conversation continuity
        -   Dynamic prompt construction based on input + retrieved docs

‚úÖ Keeps prompts structured and extensible.

‚öôÔ∏è 5. Tool & Agent Support (Advanced)
    For more advanced use cases, LangChain can:

        -   Add agents that decide what tools (search, calculator, database) to use.
        -   Integrate multiple steps like search ‚Üí read ‚Üí summarize ‚Üí reply.

üß± Summary of LangChain‚Äôs Role in RAG

üìÑ Document Ingestion	---->       Load, split, clean, store
üî° Embedding	        ---->       Plug into any embedding model
üß† Retrieval	        ---->       Query vector DBs easily
üí¨ Generation	        ---->       Format and run prompts with LLMs
üß† Agents (optional)	---->       Add reasoning or tool-using chains

LangChain helps you stitch together the RAG pipeline end-to-end, making it easy to connect [documents], [embeddings], [vector stores], and [LLMs] into a [working app] ‚Äî without having to write all the glue code yourself. Think of it as the ‚Äúmiddleware‚Äù for modern AI apps.

########################################################################################################################







Ask ChatGPT
